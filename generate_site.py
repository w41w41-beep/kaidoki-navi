import json
import math
import os
import shutil
import requests
import time
from datetime import date

# 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®å•†å“æ•°ã‚’å®šç¾©
# ã“ã®å€¤ã¯ã€AIãŒé¸åˆ¥ã—ãŸæ³¨ç›®ã‚¢ã‚¤ãƒ†ãƒ ã®æœ€å¤§è¡¨ç¤ºæ•°ã¨ãªã‚Šã¾ã™ã€‚
PRODUCTS_PER_PAGE = 10

def generate_ai_analysis(product_name, product_description, main_category):
    """OpenAI APIã‚’ä½¿ç”¨ã—ã¦å•†å“åˆ†æã®æ–‡ç« ã¨æ³¨ç›®ã‚¢ã‚¤ãƒ†ãƒ åˆ¤å®šã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        print("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        return "AIã«ã‚ˆã‚‹ä¾¡æ ¼åˆ†æã¯è¿‘æ—¥å…¬é–‹ï¼", False, "AIã«ã‚ˆã‚‹ä¾¡æ ¼åˆ†æã¯è¿‘æ—¥å…¬é–‹ï¼"
    
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿æ•´ã—ã¦ã€2ã¤ã®å½¹å‰²ã‚’æ˜ç¢ºã«æŒ‡ç¤º
    system_prompt = "ã‚ãªãŸã¯ã€å¤§æ‰‹å®¶é›»é‡è²©åº—ã®åº—å“¡ã¨ã—ã¦ã€ãŠå®¢æ§˜ã«ãŠã™ã™ã‚ã®å•†å“ã‚’èª¬æ˜ã™ã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚å°‚é–€çš„ã§ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±ã‚’ã€è¦ªã—ã¿ã‚„ã™ãç°¡æ½”ã«ã€é­…åŠ›çš„ãªè¨€è‘‰ã§ä¼ãˆã¦ãã ã•ã„ã€‚ç‰¹ã«ã€ã€Œä»ŠãŒè²·ã„æ™‚ã€ã§ã‚ã‚‹ã“ã¨ã‚’å¼·èª¿ã—ã€å…·ä½“çš„ãªã‚»ãƒ¼ãƒ«ã‚¹ãƒã‚¤ãƒ³ãƒˆã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚èªå°¾ã¯ã€Œã§ã™ã€ã‚„ã€Œã¾ã™ã€èª¿ã«ã—ã¦ãã ã•ã„ã€‚ä¾¡æ ¼ã‚„å‰²å¼•ç‡ã€ãƒã‚¤ãƒ³ãƒˆã®æƒ…å ±ãŒãªã‘ã‚Œã°ã€ãã®å•†å“è‡ªä½“ãŒæŒã¤é­…åŠ›ã‚„æ©Ÿèƒ½ã€å“è³ªã‚’å¼·èª¿ã—ã¦ãã ã•ã„ã€‚"

    # AIã«ç”Ÿæˆã•ã›ã‚‹å†…å®¹ã®æŒ‡ç¤º
    user_prompt = f"""ä»¥ä¸‹ã®å•†å“ã«ã¤ã„ã¦ã€ãŠå®¢æ§˜ã«ã€Œä»ŠãŒè²·ã„æ™‚ã ï¼ã€ã¨æ€ã‚ã›ã‚‹ã‚ˆã†ãªã€æœ€ã‚‚é‡è¦ãªã‚»ãƒ¼ãƒ«ã‚¹ãƒã‚¤ãƒ³ãƒˆã‚’**1ã¤ã ã‘**æ•™ãˆã¦ãã ã•ã„ã€‚
ã“ã®ãƒã‚¤ãƒ³ãƒˆã¯ã€ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã§ä¸€ç›®ã§ã‚ã‹ã‚‹ã‚ˆã†ã«ã€ç°¡æ½”ãªä¸€è¨€ã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ä¾‹:
ãƒ»AIåˆ†æã®çµæœ

æ¬¡ã«ã€ã“ã®å•†å“ãŒãªãœè²·ã„æ™‚ãªã®ã‹ã€ãªãœé­…åŠ›çš„ãªã®ã‹ã‚’ã€ã‚ˆã‚Šè©³ã—ãã€ç°¡æ½”ã§èª­ã¿ã‚„ã™ã„è¦ç´„ã¨ã—ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

æœ€å¾Œã«ã€ã“ã®å•†å“ãŒ**æ³¨ç›®ã‚¢ã‚¤ãƒ†ãƒ **ã§ã‚ã‚‹ã‹ã©ã†ã‹ã‚’ã€**ã¯ã„**ã‹**ã„ã„ãˆ**ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
åˆ¤æ–­åŸºæº–ã¯ä»¥ä¸‹ã®ç‚¹ã‚’ç·åˆçš„ã«è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚
- å•†å“åã‚„èª¬æ˜ã«ã€Œæœ€æ–°ã€ã€Œæ–°ãƒ¢ãƒ‡ãƒ«ã€ã€Œ2024å¹´ãƒ¢ãƒ‡ãƒ«ã€ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã€‚
- å•†å“ã«é­…åŠ›çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´ãŒã‚ã‚‹ã‹ï¼ˆå•†å“èª¬æ˜ã‹ã‚‰æ¨æ¸¬ï¼‰ã€‚
- å‰²å¼•ã‚„ãƒã‚¤ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãªã©ã€ãŠå¾—æ„ŸãŒã‚ã‚‹ã‹ï¼ˆå•†å“èª¬æ˜ã‹ã‚‰æ¨æ¸¬ï¼‰ã€‚

---
å•†å“å: {product_name}
ã‚«ãƒ†ã‚´ãƒª: {main_category}
å•†å“èª¬æ˜: {product_description}
"""
    
    payload = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "max_tokens": 400,
        "temperature": 0.7
    }

    retries = 3
    for i in range(retries):
        try:
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
            data = response.json()
            text_result = data['choices'][0]['message']['content'].strip()

            lines = text_result.split('\n')
            
            # æ³¨ç›®ã‚¢ã‚¤ãƒ†ãƒ åˆ¤å®šã®è¡Œã‚’ç‰¹å®š
            is_attention_item_result = "ã„ã„ãˆ"
            long_analysis = ""
            short_analysis = "AIã«ã‚ˆã‚‹åˆ†æã¯è¿‘æ—¥å…¬é–‹ï¼"

            # æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆã¨è©³ç´°åˆ†æã‚’åˆ†é›¢
            # æœ€åˆã®è¡Œã‚’çŸ­ã„åˆ†æã€æ®‹ã‚Šã‚’é•·ã„åˆ†æã¨ã™ã‚‹
            if lines:
                short_analysis = lines[0].strip().replace("- ", "").replace("ãƒ»", "")
                long_analysis = "\n".join(lines[1:-1]).strip()
                last_line = lines[-1].strip().lower()
                if 'ã¯ã„' in last_line:
                    is_attention_item_result = "ã¯ã„"
            
            # ã‚‚ã—é•·ã„åˆ†æãŒãªã‘ã‚Œã°çŸ­ã„åˆ†æã‚’ãã®ã¾ã¾ä½¿ã†
            if not long_analysis:
                long_analysis = short_analysis
            
            return short_analysis, (is_attention_item_result == "ã¯ã„"), long_analysis

        except requests.exceptions.RequestException as e:
            print(f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ ({i+1}/{retries}): {e}")
            time.sleep(2 ** i)
        except (IndexError, KeyError) as e:
            print(f"APIå¿œç­”ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return "AIã«ã‚ˆã‚‹ä¾¡æ ¼åˆ†æã¯è¿‘æ—¥å…¬é–‹ï¼", False, "AIã«ã‚ˆã‚‹ä¾¡æ ¼åˆ†æã¯è¿‘æ—¥å…¬é–‹ï¼"
            
    print("APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
    return "AIã«ã‚ˆã‚‹ä¾¡æ ¼åˆ†æã¯è¿‘æ—¥å…¬é–‹ï¼", False, "AIã«ã‚ˆã‚‹ä¾¡æ ¼åˆ†æã¯è¿‘æ—¥å…¬é–‹ï¼"

def fetch_rakuten_items():
    """æ¥½å¤©APIã‹ã‚‰è¤‡æ•°ã®ã‚«ãƒ†ã‚´ãƒªã§å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°"""
    app_id = os.environ.get('RAKUTEN_API_KEY')
    if not app_id:
        print("RAKUTEN_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return []

    keywords = ['ãƒ‘ã‚½ã‚³ãƒ³', 'å®¶é›»']
    all_products = []

    for keyword in keywords:
        # æ–°ç€é †ã§æ¤œç´¢
        url = f"https://app.rakuten.co.jp/services/api/IchibaItem/Search/20170706?applicationId={app_id}&keyword={keyword}&format=json&sort=+itemCreatedAt&hits={PRODUCTS_PER_PAGE}"

        try:
            response = requests.get(url)
            response.raise_for_status()
            data = response.json()
            items = data.get('Items', [])
            
            for item in items:
                item_data = item['Item']
                genre_name = item_data.get('genreName', '')
                main_cat = keyword
                
                all_products.append({
                    "id": item_data['itemCode'],
                    "name": item_data['itemName'],
                    "price": f"{int(item_data['itemPrice']):,}",
                    "image_url": item_data['mediumImageUrls'][0]['imageUrl'],
                    "rakuten_url": item_data['itemUrl'],
                    "yahoo_url": "https://shopping.yahoo.co.jp/",
                    "amazon_url": "https://www.amazon.co.jp/ref=as_li_ss_il?ie=UTF8&linkCode=ilc&tag=soc07-22&linkId=db3c1808e6f1f516353d266e76811a7c&language=ja_JP",
                    "page_url": f"pages/{item_data['itemCode']}.html",
                    "category": {
                        "main": main_cat,
                        "sub": genre_name
                    },
                    "short_ai_analysis": "placeholder", # æ–°ã—ã„çŸ­ã„åˆ†æç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    "long_ai_analysis": "placeholder",  # æ–°ã—ã„é•·ã„åˆ†æç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    "description": item_data.get('itemCaption', 'å•†å“èª¬æ˜ã¯ç¾åœ¨æº–å‚™ä¸­ã§ã™ã€‚'),
                    "date": date.today().isoformat(),
                    "main_ec_site": "æ¥½å¤©",
                    "is_attention_item": False
                })
        except requests.exceptions.RequestException as e:
            print(f"æ¥½å¤©APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return all_products

def fetch_yahoo_items():
    """Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°APIã‹ã‚‰å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°"""
    app_id = os.environ.get('YAHOO_API_KEY')
    if not app_id:
        print("YAHOO_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return []

    keywords = ['æƒé™¤æ©Ÿ', 'ã‚¤ãƒ¤ãƒ›ãƒ³']
    all_products = []
    
    for keyword in keywords:
        # æ–°ç€é †ã§æ¤œç´¢
        url = f"https://shopping.yahooapis.jp/ShoppingWebService/V3/itemSearch?appid={app_id}&query={keyword}&sort=create_datetime&hits={PRODUCTS_PER_PAGE}"
        
        try:
            response = requests.get(url)
            response.raise_for_status()
            data = response.json()
            items = data.get('hits', [])
            
            for item in items:
                all_products.append({
                    "id": item['jan_code'],
                    "name": item['name'],
                    "price": f"{int(item['price']):,}",
                    "image_url": item['image']['medium'],
                    "rakuten_url": "https://www.rakuten.co.jp/",
                    "yahoo_url": item['url'],
                    "amazon_url": "https://www.amazon.co.jp/ref=as_li_ss_il?ie=UTF8&linkCode=ilc&tag=soc07-22&linkId=db3c1808e6f1f516353d266e76811a7c&language=ja_JP",
                    "page_url": f"pages/{item['jan_code']}.html",
                    "category": {
                        "main": keyword,
                        "sub": item.get('category_name', '')
                    },
                    "short_ai_analysis": "placeholder", # æ–°ã—ã„çŸ­ã„åˆ†æç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    "long_ai_analysis": "placeholder",  # æ–°ã—ã„é•·ã„åˆ†æç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    "description": item.get('description', 'å•†å“èª¬æ˜ã¯ç¾åœ¨æº–å‚™ä¸­ã§ã™ã€‚'),
                    "date": date.today().isoformat(),
                    "main_ec_site": "Yahoo!",
                    "is_attention_item": False
                })
        except requests.exceptions.RequestException as e:
            print(f"Yahoo! APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            
    return all_products

def update_products_json(new_products):
    """æ–°ã—ã„å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æ—¢å­˜ã®products.jsonã«çµ±åˆãƒ»æ›´æ–°ã™ã‚‹é–¢æ•°"""
    try:
        if os.path.exists('products.json'):
            with open('products.json', 'r', encoding='utf-8') as f:
                existing_products = json.load(f)
        else:
            existing_products = []
    except json.JSONDecodeError:
        print("products.jsonãŒç ´æã—ã¦ã„ã‚‹ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
        existing_products = []

    updated_products = {p['id']: p for p in existing_products}
    for new_product in new_products:
        updated_products[new_product['id']] = new_product
    
    final_products = list(updated_products.values())
    
    with open('products.json', 'w', encoding='utf-8') as f:
        json.dump(final_products, f, ensure_ascii=False, indent=4)
    
    print(f"products.jsonãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚ç¾åœ¨ {len(final_products)} å€‹ã®å•†å“ã‚’è¿½è·¡ä¸­ã§ã™ã€‚")
    return final_products

def generate_site(products):
    """products.jsonã‚’èª­ã¿è¾¼ã¿ã€HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    today = date.today().isoformat()
    for product in products:
        if 'date' not in product:
            product['date'] = today
    products.sort(key=lambda p: p['date'], reverse=True)
    categories = {}
    for product in products:
        main_cat = product['category']['main']
        sub_cat = product['category']['sub']
        if main_cat not in categories:
            categories[main_cat] = []
        if sub_cat and sub_cat not in categories[main_cat]:
            categories[main_cat].append(sub_cat)
    sorted_main_cats = sorted(categories.keys())

    def generate_header_footer(current_path, sub_cat_links=None, page_title="ãŠå¾—ãªè²·ã„æ™‚ã‚’è¦‹ã¤ã‘ã‚ˆã†ï¼"):
        if "pages" in current_path:
            base_path = ".."
        elif "category" in current_path:
            base_path = "../.."
        elif "tags" in current_path:
            base_path = ".."
        else:
            base_path = "."
        main_links_html = f'<a href="{base_path}/tags/index.html">ã‚¿ã‚°ã‹ã‚‰æ¢ã™</a><span class="separator">|</span>'
        for mc_link in sorted_main_cats:
            main_links_html += f'<a href="{base_path}/category/{mc_link}/index.html">{mc_link}</a><span class="separator">|</span>'
        header_html = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ã‚«ã‚¤ãƒ‰ã‚­-ãƒŠãƒ“ | {page_title}</title>
    <link rel="stylesheet" href="{base_path}/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <meta name="google-site-verification" content="OmUuOjcxi7HXBKe47sd0WPbzCfbCOFbPj_iueHBk2qo" />
</head>
<body>
    <header>
        <div class="container">
            <h1><a href="{base_path}/index.html">ã‚«ã‚¤ãƒ‰ã‚­-ãƒŠãƒ“</a></h1>
            <p>ãŠå¾—ãªè²·ã„æ™‚ã‚’è¦‹ã¤ã‘ã‚ˆã†ï¼</p>
        </div>
    </header>

    <div class="search-bar">
        <div class="search-container">
            <input type="text" placeholder="å•†å“åã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢...">
            <button class="search-button">ğŸ”</button>
        </div>
    </div>

    <div class="genre-links-container">
        <div class="genre-links">
            {main_links_html}
        </div>
    </div>
"""
        sub_cat_links_html = ""
        if sub_cat_links:
            sub_cat_links_html += '<div class="genre-links sub-genre-links">'
            for sub_cat_link in sorted(sub_cat_links):
                sub_cat_links_html += f'<a href="{sub_cat_link.replace(" ", "")}.html">{sub_cat_link}</a><span class="separator">|</span>'
            sub_cat_links_html += '</div>'
            header_html += f"""
    <div class="sub-genre-links-container">
        {sub_cat_links_html}
    </div>
"""
        footer_html = f"""
    </main>
    <footer>
        <p>&copy; 2025 ã‚«ã‚¤ãƒ‰ã‚­-ãƒŠãƒ“. All Rights Reserved.</p>
        <div class="footer-links">
            <a href="{base_path}/privacy.html">ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼</a>
            <a href="{base_path}/disclaimer.html">å…è²¬äº‹é …</a>
            <a href="{base_path}/contact.html">ãŠå•ã„åˆã‚ã›</a>
        </div>
    </footer>
    <script src="{base_path}/script.js"></script>
</body>
</html>
        """
        return header_html, footer_html

    for root, dirs, files in os.walk('.'):
        for file in files:
            if file.endswith('.html') and not file in ['privacy.html', 'disclaimer.html', 'contact.html']:
                os.remove(os.path.join(root, file))
    if os.path.exists('category'):
        shutil.rmtree('category')
    if os.path.exists('pages'):
        shutil.rmtree('pages')
    if os.path.exists('tags'):
        shutil.rmtree('tags')

    for main_cat, sub_cats in categories.items():
        main_cat_products = [p for p in products if p['category']['main'] == main_cat]
        page_path = f"category/{main_cat}/index.html"
        os.makedirs(os.path.dirname(page_path), exist_ok=True)
        header, footer = generate_header_footer(page_path, sub_cat_links=sub_cats, page_title=f"{main_cat}ã®å•†å“ä¸€è¦§")
        main_content_html = f"""
    <main class="container">
        <div class="ai-recommendation-section">
            <h2 class="ai-section-title">{main_cat}ã®å•†å“ä¸€è¦§</h2>
            <div class="product-grid">
            """
        products_html = ""
        for product in main_cat_products:
            link_path = os.path.relpath(product['page_url'], os.path.dirname(page_path))
            products_html += f"""
<a href="{link_path}" class="product-card">
    <img src="{product['image_url']}" alt="{product['name']}">
    <div class="product-info">
        <h3 class="product-name">{product['name'][:20] + '...' if len(product['name']) > 20 else product['name']}</h3>
        <p class="product-price">{product['price']}å††</p>
        <div class="price-status-title">ğŸ’¡æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ</div>
        <div class="price-status-content ai-analysis">{product['short_ai_analysis']}</div>
    </div>
</a>
            """
        with open(page_path, 'w', encoding='utf-8') as f:
            f.write(header + main_content_html + products_html + "</div></main>" + footer)
        print(f"category/{main_cat}/index.html ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
        for sub_cat in sub_cats:
            sub_cat_products = [p for p in products if p['category']['sub'] == sub_cat]
            sub_cat_file_name = f"{sub_cat.replace(' ', '')}.html"
            page_path = f"category/{main_cat}/{sub_cat_file_name}"
            header, footer = generate_header_footer(page_path, page_title=f"{sub_cat}ã®å•†å“ä¸€è¦§")
            main_content_html = f"""
    <main class="container">
        <div class="ai-recommendation-section">
            <h2 class="ai-section-title">{sub_cat}ã®å•†å“ä¸€è¦§</h2>
            <div class="product-grid">
            """
            products_html = ""
            for product in sub_cat_products:
                link_path = os.path.relpath(product['page_url'], os.path.dirname(page_path))
                products_html += f"""
<a href="{link_path}" class="product-card">
    <img src="{product['image_url']}" alt="{product['name']}">
    <div class="product-info">
        <h3 class="product-name">{product['name'][:20] + '...' if len(product['name']) > 20 else product['name']}</h3>
        <p class="product-price">{product['price']}å††</p>
        <div class="price-status-title">ğŸ’¡æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ</div>
        <div class="price-status-content ai-analysis">{product['short_ai_analysis']}</div>
    </div>
</a>
                """
            with open(page_path, 'w', encoding='utf-8') as f:
                f.write(header + main_content_html + products_html + "</div></main>" + footer)
            print(f"{page_path} ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")

    attention_items = [p for p in products if p.get('is_attention_item')]

    total_pages = math.ceil(len(attention_items) / PRODUCTS_PER_PAGE)
    for i in range(total_pages):
        start_index = i * PRODUCTS_PER_PAGE
        end_index = start_index + PRODUCTS_PER_PAGE
        paginated_products = attention_items[start_index:end_index]
        page_num = i + 1
        page_path = 'index.html' if page_num == 1 else f'pages/page{page_num}.html'
        if page_num > 1:
            os.makedirs(os.path.dirname(page_path), exist_ok=True)
        header, footer = generate_header_footer(page_path)
        
        main_content_html = ""
        # æ³¨ç›®ã‚¢ã‚¤ãƒ†ãƒ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«è¿½åŠ 
        attention_html = ""
        if attention_items:
            attention_html += """
            <div class="ai-recommendation-section">
                <h2 class="ai-section-title">AIãŒé¸åˆ¥ã—ãŸæ³¨ç›®ã‚¢ã‚¤ãƒ†ãƒ </h2>
                <div class="product-grid">
            """
            for product in paginated_products:
                link_path = os.path.relpath(product['page_url'], os.path.dirname(page_path))
                attention_html += f"""
<a href="{link_path}" class="product-card">
    <img src="{product['image_url']}" alt="{product['name']}">
    <div class="product-info">
        <h3 class="product-name">{product['name'][:20] + '...' if len(product['name']) > 20 else product['name']}</h3>
        <p class="product-price">{product['price']}å††</p>
        <div class="price-status-title">ğŸ’¡æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ</div>
        <div class="price-status-content ai-analysis">{product['short_ai_analysis']}</div>
    </div>
</a>
                """
            attention_html += "</div></div>"
        else:
            attention_html = """
            <div class="ai-recommendation-section">
                <h2 class="ai-section-title">AIãŒé¸åˆ¥ã—ãŸæ³¨ç›®ã‚¢ã‚¤ãƒ†ãƒ </h2>
                <p class="no-items-message">ç¾åœ¨ã€æ³¨ç›®ã‚¢ã‚¤ãƒ†ãƒ ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æœ€æ–°æƒ…å ±ã‚’ãŠå¾…ã¡ãã ã•ã„ï¼</p>
            </div>
            """

        main_content_html += attention_html
        
        pagination_html = ""
        if total_pages > 1:
            pagination_html += '<div class="pagination">'
            if page_num > 1:
                prev_link = 'index.html' if page_num == 2 else f'pages/page{page_num - 1}.html'
                pagination_html += f'<a href="{os.path.relpath(prev_link, os.path.dirname(page_path))}" class="prev">å‰ã¸</a>'
            for p in range(1, total_pages + 1):
                page_link = 'index.html' if p == 1 else f'pages/page{p}.html'
                active_class = 'active' if p == page_num else ''
                pagination_html += f'<a href="{os.path.relpath(page_link, os.path.dirname(page_path))}" class="{active_class}">{p}</a>'
            if page_num < total_pages:
                next_link = f'pages/page{page_num + 1}.html'
                pagination_html += f'<a href="{os.path.relpath(next_link, os.path.dirname(page_path))}" class="next">æ¬¡ã¸</a>'
            pagination_html += '</div>'

        with open(page_path, 'w', encoding='utf-8') as f:
            f.write(header + '<main class="container">' + main_content_html + '</div>' + pagination_html + '</main>' + footer)
        print(f"{page_path} ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")

    for product in products:
        page_path = product['page_url']
        dir_name = os.path.dirname(page_path)
        if dir_name:
            os.makedirs(dir_name, exist_ok=True)
        header, footer = generate_header_footer(page_path, page_title=f"{product['name']}ã®è²·ã„æ™‚æƒ…å ±")
        ai_analysis_block_html = f"""
            <div class="ai-analysis-block">
                <div class="ai-analysis-text">
                    <h2>AIã«ã‚ˆã‚‹è²·ã„æ™‚åˆ†æ</h2>
                    <p class="long-analysis-text">{product['long_ai_analysis']}</p>
                </div>
            </div>
        """
        specs_html = ""
        if "specs" in product:
            specs_html = f"""
                <div class="item-specs">
                    <h2>è£½å“ä»•æ§˜ãƒ»ã‚¹ãƒšãƒƒã‚¯</h2>
                    <p>{product.get('specs', '')}</p>
                </div>
            """
        purchase_button_html = ""
        main_ec_site = product.get("main_ec_site")
        
        if main_ec_site == "Amazon":
            purchase_button_html = f'<a href="{product["amazon_url"]}" class="purchase-button" target="_blank">Amazonã§è³¼å…¥ã™ã‚‹</a>'
        elif main_ec_site == "æ¥½å¤©":
            purchase_button_html = f'<a href="{product["rakuten_url"]}" class="purchase-button" target="_blank">æ¥½å¤©å¸‚å ´ã§è³¼å…¥ã™ã‚‹</a>'
        elif main_ec_site == "Yahoo!":
            purchase_button_html = f'<a href="{product["yahoo_url"]}" class="purchase-button" target="_blank">Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã§è³¼å…¥ã™ã‚‹</a>'

        affiliate_links_html = f"""
            <div class="lowest-price-section">
                <p class="lowest-price-label">æœ€å®‰å€¤ã‚·ãƒ§ãƒƒãƒ—ã‚’ãƒã‚§ãƒƒã‚¯ï¼</p>
                <div class="lowest-price-buttons">
                    <a href="{product.get("amazon_url", "https://www.amazon.co.jp/")}" class="btn shop-link" target="_blank">Amazonã§è¦‹ã‚‹</a>
                    <a href="{product.get("rakuten_url", "https://www.rakuten.co.jp/")}" class="btn shop-link" target="_blank">æ¥½å¤©å¸‚å ´ã§è¦‹ã‚‹</a>
                    <a href="{product.get("yahoo_url", "https://shopping.yahoo.co.jp/")}" class="btn shop-link" target="_blank">Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã§è¦‹ã‚‹</a>
                </div>
            </div>
        """
        item_html_content = f"""
<main class="container">
    <div class="product-detail">
        <div class="item-detail">
            <div class="item-image">
                <img src="{product['image_url']}" alt="{product['name']}" class="main-product-image">
            </div>
            <div class="item-info">
                <h1 class="item-name">{product['name']}</h1>
                <p class="item-category">ã‚«ãƒ†ã‚´ãƒªï¼š<a href="{os.path.relpath('category/' + product['category']['main'] + '/index.html', os.path.dirname(page_path))}">{product['category']['main']}</a> &gt;
                <a href="{os.path.relpath('category/' + product['category']['main'] + '/' + product['category']['sub'].replace(' ', '') + '.html', os.path.dirname(page_path))}">{product['category']['sub']}</a></p>
                <div class="price-section">
                    <p class="current-price">ç¾åœ¨ã®ä¾¡æ ¼ï¼š<span>{product['price']}</span>å††</p>
                </div>
                <div class="ai-recommendation-section">
                    <div class="price-status-title">ğŸ’¡æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ</div>
                    <div class="price-status-content ai-analysis">{product['short_ai_analysis']}</div>
                    {purchase_button_html}
                </div>
                {ai_analysis_block_html}
                {affiliate_links_html}
                <div class="item-description">
                    <h2>å•†å“èª¬æ˜</h2>
                    <p>{product.get('description', 'å•†å“èª¬æ˜ã¯ç¾åœ¨æº–å‚™ä¸­ã§ã™ã€‚')}</p>
                </div>
                {specs_html}
            </div>
        </div>
    </div>
</main>
"""
        with open(page_path, 'w', encoding='utf-8') as f:
            f.write(header + item_html_content + footer)
        print(f"{page_path} ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
    
    def create_sitemap():
        base_url = "https://w41w41-beep.github.io/kaidoki-navi/"
        sitemap_content = '<?xml version="1.0" encoding="UTF-8"?>\n'
        sitemap_content += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
        sitemap_content += '  <url>\n'
        sitemap_content += f'    <loc>{base_url}</loc>\n'
        sitemap_content += f'    <lastmod>{date.today().isoformat()}</lastmod>\n'
        sitemap_content += '    <changefreq>daily</changefreq>\n'
        sitemap_content += '    <priority>1.0</priority>\n'
        sitemap_content += '  </url>\n'
        categories = {}
        for product in products:
            main_cat = product['category']['main']
            sub_cat = product['category']['sub']
            if main_cat not in categories:
                categories[main_cat] = set()
            categories[main_cat].add(sub_cat)
        for main_cat, sub_cats in categories.items():
            sitemap_content += '  <url>\n'
            sitemap_content += f'    <loc>{base_url}category/{main_cat}/index.html</loc>\n'
            sitemap_content += f'    <lastmod>{date.today().isoformat()}</lastmod>\n'
            sitemap_content += '    <changefreq>daily</changefreq>\n'
            sitemap_content += '    <priority>0.8</priority>\n'
            sitemap_content += '  </url>\n'
            for sub_cat in sub_cats:
                sitemap_content += '  <url>\n'
                sitemap_content += f'    <loc>{base_url}category/{main_cat}/{sub_cat.replace(" ", "")}.html</loc>\n'
                sitemap_content += f'    <lastmod>{date.today().isoformat()}</lastmod>\n'
                sitemap_content += '    <changefreq>daily</changefreq>\n'
                sitemap_content += '    <priority>0.7</priority>\n'
                sitemap_content += '  </url>\n'
        for product in products:
            sitemap_content += '  <url>\n'
            sitemap_content += f'    <loc>{base_url}{product["page_url"]}</loc>\n'
            sitemap_content += f'    <lastmod>{date.today().isoformat()}</lastmod>\n'
            sitemap_content += '    <changefreq>daily</changefreq>\n'
            sitemap_content += '    <priority>0.6</priority>\n'
            sitemap_content += '  </url>\n'
        static_pages = ["privacy.html", "disclaimer.html", "contact.html"]
        for page in static_pages:
            sitemap_content += '  <url>\n'
            sitemap_content += f'    <loc>{base_url}{page}</loc>\n'
            sitemap_content += f'    <lastmod>{date.today().isoformat()}</lastmod>\n'
            sitemap_content += '    <changefreq>monthly</changefreq>\n'
            sitemap_content += '    <priority>0.5</priority>\n'
            sitemap_content += '  </url>\n'
        sitemap_content += '</urlset>'
        with open('sitemap.xml', 'w', encoding='utf-8') as f:
            f.write(sitemap_content)
        print("sitemap.xml ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
    create_sitemap()
    print("ã‚µã‚¤ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    rakuten_products = fetch_rakuten_items()
    yahoo_products = fetch_yahoo_items()
    
    new_products = rakuten_products + yahoo_products
    
    print("æ–°ã—ã„å•†å“ã®AIåˆ†æã‚’ç”Ÿæˆä¸­ã§ã™...")
    for product in new_products:
        product['short_ai_analysis'], product['is_attention_item'], product['long_ai_analysis'] = generate_ai_analysis(product['name'], product['description'], product['category']['main'])
        time.sleep(1)
    
    products = update_products_json(new_products)
    generate_site(products)
